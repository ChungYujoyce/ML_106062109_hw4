{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from math import exp\n",
    "# process data label as 0, 1, 2 for training\n",
    "def loadDataset(dataset): \n",
    "    newdata = []\n",
    "    for x in range(len(dataset)-1):\n",
    "        for i in range(0,len(dataset[x]),4):\n",
    "            if dataset[x][i] == \"I\": # encounter labels, change it\n",
    "                if dataset[x][-3:-1] == \"sa\": # Iris-setosa\n",
    "                    newdata.append(0)\n",
    "                elif dataset[x][-2:-1] == \"r\": # Iris-versicolor\n",
    "                    newdata.append(1)\n",
    "                elif dataset[x][-3:-1] == \"ca\": # Iris-virginica\n",
    "                    newdata.append(2) \n",
    "                break\n",
    "            else:\n",
    "                attribute = float(dataset[x][i:i+3])\n",
    "                newdata.append(attribute)\n",
    "        trainingSet.append(newdata)       \n",
    "        newdata = [] # clear the package\n",
    "    return trainingSet\n",
    "\n",
    "# find the range of data to do normalize\n",
    "def dataset_minmax(dataset): # zip a(1,2,3) , b(4,5,6) to [(1,4), (2,5), (3,6)] \n",
    "    minmax = list()\n",
    "    stats = [[min(column), max(column)] for column in zip(*dataset)] # unzip the file\n",
    "    return stats\n",
    "\n",
    "# rescale data to range 0~1\n",
    "def normalize_data(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)-1):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "# calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    "\n",
    "# transfer neuron activation \n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + exp(-activation))\n",
    "\n",
    "# calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)\n",
    "\n",
    "#forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs # keep updating the neurons\n",
    "    return inputs\n",
    "\n",
    "# backpropagate error and sotre in neurons\n",
    "def backward_propagate_error(network, expected): \n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)): # hidden\n",
    "                error = 0.0\n",
    "                for neuron in network[i+1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error) \n",
    "        else:\n",
    "            for j in range(len(layer)): # output\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "\n",
    "# update network weights with error\n",
    "def update_weights(network, row, learning_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i-1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += learning_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += learning_rate * neuron['delta']\n",
    "\n",
    "def train_network(network, train, learning_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1 # one hot encoding !!!\n",
    "            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, learning_rate)\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, learning_rate, sum_error))\n",
    "\n",
    "\n",
    "def init_network(n_inputs, n_hidden, n_outputs): \n",
    "# create n_hidden neurons and each neuron in the hidden layer has n_inputs + 1 weights\n",
    "    network = list()\n",
    "    hidden_layer1 = [{'weights': [(random.random()-0.5)/5.0 for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer1) # 4 neurons with 4 input weights plus the bias\n",
    "    hidden_layer2 = [{'weights': [(random.random()-0.5)/5.0 for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer2)\n",
    "    #for layer in network:\n",
    "        #print(layer)\n",
    "    output_layer = [{'weights': [(random.random()-0.5)/5.0 for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    #print(\"out\",hidden_layer)\n",
    "    network.append(output_layer) # 3 neurons with 4 input weights plus the bias\n",
    "    #for layer in network:\n",
    "     #   print(layer)\n",
    "    return network\n",
    "\n",
    "# make a prediction with a network\n",
    "# It returns the index in the network output that has the largest probability. \n",
    "# Assuming that class values have been converted to integers starting at 0. [0,1,2]\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))\n",
    "\n",
    "# backpropagation with stochastic gradient descent\n",
    "def back_propagate(train, learning_rate, n_epoch, n_hidden):\n",
    "    n_inputs = len(train[0]) - 1\n",
    "    n_outputs = len(set([row[-1] for row in train]))\n",
    "    network = init_network(n_inputs, n_hidden, n_outputs)\n",
    "    train_network(network, train, learning_rate, n_epoch, n_outputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet=[]\n",
    "testSet=[]\n",
    "\n",
    "f = open('iris.data.txt', \"r\")\n",
    "lines = f.readlines()\n",
    "dataset = list(lines)\n",
    "trainingSet = loadDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'weights': [0.09476963274630996, 0.053027788721791126, 0.0640127414916201, -0.036535701926091305, 0.09557854129117668]}, {'weights': [-0.046750625306987106, -0.07544001942998857, 0.014079895776819007, -0.04803823438471035, 0.012982657166973732]}, {'weights': [0.07706345342416832, 0.02822224681479113, 0.011876092984084962, 0.09177451708898958, -0.021624807774263767]}, {'weights': [-0.06945978746778154, 0.04462137087349809, -0.038428092296978814, -0.07105622063044485, 0.043804117157960176]}]\n",
      "[{'weights': [-0.006089427940792635, 0.08231725009076343, 0.07180014594329653, 0.03457910979667846, -0.0022939581317449774]}, {'weights': [0.04124919758156174, -0.07689027387827982, 0.027900725777482906, -0.014305730638683567, -0.002977628217086048]}, {'weights': [-0.08057977522116935, -0.03989567332825108, 0.019288094503728037, -0.04760216623988922, 0.09128766062942868]}, {'weights': [0.047293670549096015, 0.06766946147971445, 0.02773247308070992, -0.08919260752571634, 0.0911709296183553]}]\n",
      "[{'weights': [0.08237106777256815, 0.07440262930801174, -0.046565564685059437, 0.07466768539785078, 0.008909179643349719]}, {'weights': [0.01034582889780582, 0.03044035057206036, 0.07905794610834356, -0.09109110366683448, 0.027836528012030827]}, {'weights': [-0.07476856816150271, -0.080863101551149, 0.0601680241436233, 0.025158174815862133, 0.04271500622503155]}]\n",
      ">epoch=0, lrate=0.100, error=93.144\n",
      ">epoch=1, lrate=0.100, error=95.215\n",
      ">epoch=2, lrate=0.100, error=96.602\n",
      ">epoch=3, lrate=0.100, error=96.939\n",
      ">epoch=4, lrate=0.100, error=97.031\n",
      ">epoch=5, lrate=0.100, error=97.066\n",
      ">epoch=6, lrate=0.100, error=97.088\n",
      ">epoch=7, lrate=0.100, error=97.107\n",
      ">epoch=8, lrate=0.100, error=97.126\n",
      ">epoch=9, lrate=0.100, error=97.143\n",
      ">epoch=10, lrate=0.100, error=97.161\n",
      ">epoch=11, lrate=0.100, error=97.179\n",
      ">epoch=12, lrate=0.100, error=97.196\n",
      ">epoch=13, lrate=0.100, error=97.213\n",
      ">epoch=14, lrate=0.100, error=97.230\n",
      ">epoch=15, lrate=0.100, error=97.247\n",
      ">epoch=16, lrate=0.100, error=97.264\n",
      ">epoch=17, lrate=0.100, error=97.281\n",
      ">epoch=18, lrate=0.100, error=97.297\n",
      ">epoch=19, lrate=0.100, error=97.314\n",
      ">epoch=20, lrate=0.100, error=97.330\n",
      ">epoch=21, lrate=0.100, error=97.346\n",
      ">epoch=22, lrate=0.100, error=97.362\n",
      ">epoch=23, lrate=0.100, error=97.378\n",
      ">epoch=24, lrate=0.100, error=97.394\n",
      ">epoch=25, lrate=0.100, error=97.410\n",
      ">epoch=26, lrate=0.100, error=97.425\n",
      ">epoch=27, lrate=0.100, error=97.441\n",
      ">epoch=28, lrate=0.100, error=97.456\n",
      ">epoch=29, lrate=0.100, error=97.471\n",
      ">epoch=30, lrate=0.100, error=97.486\n",
      ">epoch=31, lrate=0.100, error=97.501\n",
      ">epoch=32, lrate=0.100, error=97.516\n",
      ">epoch=33, lrate=0.100, error=97.531\n",
      ">epoch=34, lrate=0.100, error=97.545\n",
      ">epoch=35, lrate=0.100, error=97.560\n",
      ">epoch=36, lrate=0.100, error=97.574\n",
      ">epoch=37, lrate=0.100, error=97.588\n",
      ">epoch=38, lrate=0.100, error=97.602\n",
      ">epoch=39, lrate=0.100, error=97.616\n",
      ">epoch=40, lrate=0.100, error=97.630\n",
      ">epoch=41, lrate=0.100, error=97.644\n",
      ">epoch=42, lrate=0.100, error=97.658\n",
      ">epoch=43, lrate=0.100, error=97.671\n",
      ">epoch=44, lrate=0.100, error=97.685\n",
      ">epoch=45, lrate=0.100, error=97.698\n",
      ">epoch=46, lrate=0.100, error=97.712\n",
      ">epoch=47, lrate=0.100, error=97.725\n",
      ">epoch=48, lrate=0.100, error=97.738\n",
      ">epoch=49, lrate=0.100, error=97.751\n",
      ">epoch=50, lrate=0.100, error=97.764\n",
      ">epoch=51, lrate=0.100, error=97.776\n",
      ">epoch=52, lrate=0.100, error=97.789\n",
      ">epoch=53, lrate=0.100, error=97.801\n",
      ">epoch=54, lrate=0.100, error=97.814\n",
      ">epoch=55, lrate=0.100, error=97.826\n",
      ">epoch=56, lrate=0.100, error=97.838\n",
      ">epoch=57, lrate=0.100, error=97.851\n",
      ">epoch=58, lrate=0.100, error=97.863\n",
      ">epoch=59, lrate=0.100, error=97.875\n",
      ">epoch=60, lrate=0.100, error=97.886\n",
      ">epoch=61, lrate=0.100, error=97.898\n",
      ">epoch=62, lrate=0.100, error=97.910\n",
      ">epoch=63, lrate=0.100, error=97.921\n",
      ">epoch=64, lrate=0.100, error=97.933\n",
      ">epoch=65, lrate=0.100, error=97.944\n",
      ">epoch=66, lrate=0.100, error=97.955\n",
      ">epoch=67, lrate=0.100, error=97.967\n",
      ">epoch=68, lrate=0.100, error=97.978\n",
      ">epoch=69, lrate=0.100, error=97.989\n",
      ">epoch=70, lrate=0.100, error=98.000\n",
      ">epoch=71, lrate=0.100, error=98.010\n",
      ">epoch=72, lrate=0.100, error=98.021\n",
      ">epoch=73, lrate=0.100, error=98.032\n",
      ">epoch=74, lrate=0.100, error=98.042\n",
      ">epoch=75, lrate=0.100, error=98.053\n",
      ">epoch=76, lrate=0.100, error=98.063\n",
      ">epoch=77, lrate=0.100, error=98.073\n",
      ">epoch=78, lrate=0.100, error=98.084\n",
      ">epoch=79, lrate=0.100, error=98.094\n",
      ">epoch=80, lrate=0.100, error=98.104\n",
      ">epoch=81, lrate=0.100, error=98.114\n",
      ">epoch=82, lrate=0.100, error=98.123\n",
      ">epoch=83, lrate=0.100, error=98.133\n",
      ">epoch=84, lrate=0.100, error=98.143\n",
      ">epoch=85, lrate=0.100, error=98.152\n",
      ">epoch=86, lrate=0.100, error=98.162\n",
      ">epoch=87, lrate=0.100, error=98.171\n",
      ">epoch=88, lrate=0.100, error=98.181\n",
      ">epoch=89, lrate=0.100, error=98.190\n",
      ">epoch=90, lrate=0.100, error=98.199\n",
      ">epoch=91, lrate=0.100, error=98.208\n",
      ">epoch=92, lrate=0.100, error=98.217\n",
      ">epoch=93, lrate=0.100, error=98.226\n",
      ">epoch=94, lrate=0.100, error=98.235\n",
      ">epoch=95, lrate=0.100, error=98.244\n",
      ">epoch=96, lrate=0.100, error=98.252\n",
      ">epoch=97, lrate=0.100, error=98.261\n",
      ">epoch=98, lrate=0.100, error=98.269\n",
      ">epoch=99, lrate=0.100, error=98.278\n",
      ">epoch=100, lrate=0.100, error=98.286\n",
      ">epoch=101, lrate=0.100, error=98.294\n",
      ">epoch=102, lrate=0.100, error=98.303\n",
      ">epoch=103, lrate=0.100, error=98.311\n",
      ">epoch=104, lrate=0.100, error=98.319\n",
      ">epoch=105, lrate=0.100, error=98.327\n",
      ">epoch=106, lrate=0.100, error=98.335\n",
      ">epoch=107, lrate=0.100, error=98.343\n",
      ">epoch=108, lrate=0.100, error=98.350\n",
      ">epoch=109, lrate=0.100, error=98.358\n",
      ">epoch=110, lrate=0.100, error=98.366\n",
      ">epoch=111, lrate=0.100, error=98.373\n",
      ">epoch=112, lrate=0.100, error=98.381\n",
      ">epoch=113, lrate=0.100, error=98.388\n",
      ">epoch=114, lrate=0.100, error=98.395\n",
      ">epoch=115, lrate=0.100, error=98.403\n",
      ">epoch=116, lrate=0.100, error=98.410\n",
      ">epoch=117, lrate=0.100, error=98.417\n",
      ">epoch=118, lrate=0.100, error=98.424\n",
      ">epoch=119, lrate=0.100, error=98.431\n",
      ">epoch=120, lrate=0.100, error=98.438\n",
      ">epoch=121, lrate=0.100, error=98.445\n",
      ">epoch=122, lrate=0.100, error=98.452\n",
      ">epoch=123, lrate=0.100, error=98.458\n",
      ">epoch=124, lrate=0.100, error=98.465\n",
      ">epoch=125, lrate=0.100, error=98.472\n",
      ">epoch=126, lrate=0.100, error=98.478\n",
      ">epoch=127, lrate=0.100, error=98.485\n",
      ">epoch=128, lrate=0.100, error=98.491\n",
      ">epoch=129, lrate=0.100, error=98.497\n",
      ">epoch=130, lrate=0.100, error=98.504\n",
      ">epoch=131, lrate=0.100, error=98.510\n",
      ">epoch=132, lrate=0.100, error=98.516\n",
      ">epoch=133, lrate=0.100, error=98.522\n",
      ">epoch=134, lrate=0.100, error=98.528\n",
      ">epoch=135, lrate=0.100, error=98.534\n",
      ">epoch=136, lrate=0.100, error=98.540\n",
      ">epoch=137, lrate=0.100, error=98.546\n",
      ">epoch=138, lrate=0.100, error=98.552\n",
      ">epoch=139, lrate=0.100, error=98.558\n",
      ">epoch=140, lrate=0.100, error=98.563\n",
      ">epoch=141, lrate=0.100, error=98.569\n",
      ">epoch=142, lrate=0.100, error=98.575\n",
      ">epoch=143, lrate=0.100, error=98.580\n",
      ">epoch=144, lrate=0.100, error=98.586\n",
      ">epoch=145, lrate=0.100, error=98.591\n",
      ">epoch=146, lrate=0.100, error=98.597\n",
      ">epoch=147, lrate=0.100, error=98.602\n",
      ">epoch=148, lrate=0.100, error=98.607\n",
      ">epoch=149, lrate=0.100, error=98.613\n",
      ">epoch=150, lrate=0.100, error=98.618\n",
      ">epoch=151, lrate=0.100, error=98.623\n",
      ">epoch=152, lrate=0.100, error=98.628\n",
      ">epoch=153, lrate=0.100, error=98.633\n",
      ">epoch=154, lrate=0.100, error=98.638\n",
      ">epoch=155, lrate=0.100, error=98.643\n",
      ">epoch=156, lrate=0.100, error=98.648\n",
      ">epoch=157, lrate=0.100, error=98.653\n",
      ">epoch=158, lrate=0.100, error=98.657\n",
      ">epoch=159, lrate=0.100, error=98.662\n",
      ">epoch=160, lrate=0.100, error=98.667\n",
      ">epoch=161, lrate=0.100, error=98.671\n",
      ">epoch=162, lrate=0.100, error=98.676\n",
      ">epoch=163, lrate=0.100, error=98.681\n",
      ">epoch=164, lrate=0.100, error=98.685\n",
      ">epoch=165, lrate=0.100, error=98.690\n",
      ">epoch=166, lrate=0.100, error=98.694\n",
      ">epoch=167, lrate=0.100, error=98.698\n",
      ">epoch=168, lrate=0.100, error=98.703\n",
      ">epoch=169, lrate=0.100, error=98.707\n",
      ">epoch=170, lrate=0.100, error=98.711\n",
      ">epoch=171, lrate=0.100, error=98.715\n",
      ">epoch=172, lrate=0.100, error=98.719\n",
      ">epoch=173, lrate=0.100, error=98.723\n",
      ">epoch=174, lrate=0.100, error=98.727\n",
      ">epoch=175, lrate=0.100, error=98.731\n",
      ">epoch=176, lrate=0.100, error=98.735\n",
      ">epoch=177, lrate=0.100, error=98.739\n",
      ">epoch=178, lrate=0.100, error=98.743\n",
      ">epoch=179, lrate=0.100, error=98.747\n",
      ">epoch=180, lrate=0.100, error=98.750\n",
      ">epoch=181, lrate=0.100, error=98.754\n",
      ">epoch=182, lrate=0.100, error=98.758\n",
      ">epoch=183, lrate=0.100, error=98.761\n",
      ">epoch=184, lrate=0.100, error=98.765\n",
      ">epoch=185, lrate=0.100, error=98.768\n",
      ">epoch=186, lrate=0.100, error=98.772\n",
      ">epoch=187, lrate=0.100, error=98.775\n",
      ">epoch=188, lrate=0.100, error=98.778\n",
      ">epoch=189, lrate=0.100, error=98.781\n",
      ">epoch=190, lrate=0.100, error=98.785\n",
      ">epoch=191, lrate=0.100, error=98.788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=192, lrate=0.100, error=98.791\n",
      ">epoch=193, lrate=0.100, error=98.794\n",
      ">epoch=194, lrate=0.100, error=98.797\n",
      ">epoch=195, lrate=0.100, error=98.800\n",
      ">epoch=196, lrate=0.100, error=98.802\n",
      ">epoch=197, lrate=0.100, error=98.805\n",
      ">epoch=198, lrate=0.100, error=98.808\n",
      ">epoch=199, lrate=0.100, error=98.810\n",
      ">epoch=200, lrate=0.100, error=98.813\n",
      ">epoch=201, lrate=0.100, error=98.815\n",
      ">epoch=202, lrate=0.100, error=98.817\n",
      ">epoch=203, lrate=0.100, error=98.820\n",
      ">epoch=204, lrate=0.100, error=98.822\n",
      ">epoch=205, lrate=0.100, error=98.824\n",
      ">epoch=206, lrate=0.100, error=98.826\n",
      ">epoch=207, lrate=0.100, error=98.828\n",
      ">epoch=208, lrate=0.100, error=98.829\n",
      ">epoch=209, lrate=0.100, error=98.831\n",
      ">epoch=210, lrate=0.100, error=98.832\n",
      ">epoch=211, lrate=0.100, error=98.834\n",
      ">epoch=212, lrate=0.100, error=98.835\n",
      ">epoch=213, lrate=0.100, error=98.836\n",
      ">epoch=214, lrate=0.100, error=98.837\n",
      ">epoch=215, lrate=0.100, error=98.837\n",
      ">epoch=216, lrate=0.100, error=98.838\n",
      ">epoch=217, lrate=0.100, error=98.838\n",
      ">epoch=218, lrate=0.100, error=98.838\n",
      ">epoch=219, lrate=0.100, error=98.838\n",
      ">epoch=220, lrate=0.100, error=98.838\n",
      ">epoch=221, lrate=0.100, error=98.837\n",
      ">epoch=222, lrate=0.100, error=98.836\n",
      ">epoch=223, lrate=0.100, error=98.835\n",
      ">epoch=224, lrate=0.100, error=98.833\n",
      ">epoch=225, lrate=0.100, error=98.832\n",
      ">epoch=226, lrate=0.100, error=98.829\n",
      ">epoch=227, lrate=0.100, error=98.827\n",
      ">epoch=228, lrate=0.100, error=98.823\n",
      ">epoch=229, lrate=0.100, error=98.820\n",
      ">epoch=230, lrate=0.100, error=98.816\n",
      ">epoch=231, lrate=0.100, error=98.811\n",
      ">epoch=232, lrate=0.100, error=98.805\n",
      ">epoch=233, lrate=0.100, error=98.799\n",
      ">epoch=234, lrate=0.100, error=98.792\n",
      ">epoch=235, lrate=0.100, error=98.785\n",
      ">epoch=236, lrate=0.100, error=98.776\n",
      ">epoch=237, lrate=0.100, error=98.766\n",
      ">epoch=238, lrate=0.100, error=98.755\n",
      ">epoch=239, lrate=0.100, error=98.743\n",
      ">epoch=240, lrate=0.100, error=98.729\n",
      ">epoch=241, lrate=0.100, error=98.714\n",
      ">epoch=242, lrate=0.100, error=98.697\n",
      ">epoch=243, lrate=0.100, error=98.678\n",
      ">epoch=244, lrate=0.100, error=98.657\n",
      ">epoch=245, lrate=0.100, error=98.633\n",
      ">epoch=246, lrate=0.100, error=98.606\n",
      ">epoch=247, lrate=0.100, error=98.576\n",
      ">epoch=248, lrate=0.100, error=98.542\n",
      ">epoch=249, lrate=0.100, error=98.505\n",
      ">epoch=250, lrate=0.100, error=98.462\n",
      ">epoch=251, lrate=0.100, error=98.414\n",
      ">epoch=252, lrate=0.100, error=98.360\n",
      ">epoch=253, lrate=0.100, error=98.299\n",
      ">epoch=254, lrate=0.100, error=98.230\n",
      ">epoch=255, lrate=0.100, error=98.151\n",
      ">epoch=256, lrate=0.100, error=98.062\n",
      ">epoch=257, lrate=0.100, error=97.960\n",
      ">epoch=258, lrate=0.100, error=97.843\n",
      ">epoch=259, lrate=0.100, error=97.708\n",
      ">epoch=260, lrate=0.100, error=97.554\n",
      ">epoch=261, lrate=0.100, error=97.376\n",
      ">epoch=262, lrate=0.100, error=97.170\n",
      ">epoch=263, lrate=0.100, error=96.932\n",
      ">epoch=264, lrate=0.100, error=96.654\n",
      ">epoch=265, lrate=0.100, error=96.330\n",
      ">epoch=266, lrate=0.100, error=95.952\n",
      ">epoch=267, lrate=0.100, error=95.508\n",
      ">epoch=268, lrate=0.100, error=94.987\n",
      ">epoch=269, lrate=0.100, error=94.375\n",
      ">epoch=270, lrate=0.100, error=93.653\n",
      ">epoch=271, lrate=0.100, error=92.804\n",
      ">epoch=272, lrate=0.100, error=91.806\n",
      ">epoch=273, lrate=0.100, error=90.637\n",
      ">epoch=274, lrate=0.100, error=89.274\n",
      ">epoch=275, lrate=0.100, error=87.700\n",
      ">epoch=276, lrate=0.100, error=85.902\n",
      ">epoch=277, lrate=0.100, error=83.883\n",
      ">epoch=278, lrate=0.100, error=81.659\n",
      ">epoch=279, lrate=0.100, error=79.269\n",
      ">epoch=280, lrate=0.100, error=76.770\n",
      ">epoch=281, lrate=0.100, error=74.229\n",
      ">epoch=282, lrate=0.100, error=71.718\n",
      ">epoch=283, lrate=0.100, error=69.297\n",
      ">epoch=284, lrate=0.100, error=67.013\n",
      ">epoch=285, lrate=0.100, error=64.893\n",
      ">epoch=286, lrate=0.100, error=62.946\n",
      ">epoch=287, lrate=0.100, error=61.173\n",
      ">epoch=288, lrate=0.100, error=59.565\n",
      ">epoch=289, lrate=0.100, error=58.108\n",
      ">epoch=290, lrate=0.100, error=56.791\n",
      ">epoch=291, lrate=0.100, error=55.598\n",
      ">epoch=292, lrate=0.100, error=54.517\n",
      ">epoch=293, lrate=0.100, error=53.536\n",
      ">epoch=294, lrate=0.100, error=52.645\n",
      ">epoch=295, lrate=0.100, error=51.832\n",
      ">epoch=296, lrate=0.100, error=51.090\n",
      ">epoch=297, lrate=0.100, error=50.410\n",
      ">epoch=298, lrate=0.100, error=49.786\n",
      ">epoch=299, lrate=0.100, error=49.211\n",
      ">epoch=300, lrate=0.100, error=48.680\n",
      ">epoch=301, lrate=0.100, error=48.187\n",
      ">epoch=302, lrate=0.100, error=47.728\n",
      ">epoch=303, lrate=0.100, error=47.299\n",
      ">epoch=304, lrate=0.100, error=46.897\n",
      ">epoch=305, lrate=0.100, error=46.518\n",
      ">epoch=306, lrate=0.100, error=46.160\n",
      ">epoch=307, lrate=0.100, error=45.821\n",
      ">epoch=308, lrate=0.100, error=45.497\n",
      ">epoch=309, lrate=0.100, error=45.188\n",
      ">epoch=310, lrate=0.100, error=44.890\n",
      ">epoch=311, lrate=0.100, error=44.604\n",
      ">epoch=312, lrate=0.100, error=44.326\n",
      ">epoch=313, lrate=0.100, error=44.055\n",
      ">epoch=314, lrate=0.100, error=43.791\n",
      ">epoch=315, lrate=0.100, error=43.530\n",
      ">epoch=316, lrate=0.100, error=43.273\n",
      ">epoch=317, lrate=0.100, error=43.018\n",
      ">epoch=318, lrate=0.100, error=42.763\n",
      ">epoch=319, lrate=0.100, error=42.507\n",
      ">epoch=320, lrate=0.100, error=42.249\n",
      ">epoch=321, lrate=0.100, error=41.988\n",
      ">epoch=322, lrate=0.100, error=41.721\n",
      ">epoch=323, lrate=0.100, error=41.449\n",
      ">epoch=324, lrate=0.100, error=41.169\n",
      ">epoch=325, lrate=0.100, error=40.881\n",
      ">epoch=326, lrate=0.100, error=40.583\n",
      ">epoch=327, lrate=0.100, error=40.275\n",
      ">epoch=328, lrate=0.100, error=39.955\n",
      ">epoch=329, lrate=0.100, error=39.623\n",
      ">epoch=330, lrate=0.100, error=39.278\n",
      ">epoch=331, lrate=0.100, error=38.919\n",
      ">epoch=332, lrate=0.100, error=38.545\n",
      ">epoch=333, lrate=0.100, error=38.155\n",
      ">epoch=334, lrate=0.100, error=37.749\n",
      ">epoch=335, lrate=0.100, error=37.325\n",
      ">epoch=336, lrate=0.100, error=36.883\n",
      ">epoch=337, lrate=0.100, error=36.421\n",
      ">epoch=338, lrate=0.100, error=35.940\n",
      ">epoch=339, lrate=0.100, error=35.436\n",
      ">epoch=340, lrate=0.100, error=34.911\n",
      ">epoch=341, lrate=0.100, error=34.364\n",
      ">epoch=342, lrate=0.100, error=33.793\n",
      ">epoch=343, lrate=0.100, error=33.199\n",
      ">epoch=344, lrate=0.100, error=32.582\n",
      ">epoch=345, lrate=0.100, error=31.942\n",
      ">epoch=346, lrate=0.100, error=31.280\n",
      ">epoch=347, lrate=0.100, error=30.599\n",
      ">epoch=348, lrate=0.100, error=29.898\n",
      ">epoch=349, lrate=0.100, error=29.181\n",
      ">epoch=350, lrate=0.100, error=28.450\n",
      ">epoch=351, lrate=0.100, error=27.707\n",
      ">epoch=352, lrate=0.100, error=26.956\n",
      ">epoch=353, lrate=0.100, error=26.199\n",
      ">epoch=354, lrate=0.100, error=25.440\n",
      ">epoch=355, lrate=0.100, error=24.682\n",
      ">epoch=356, lrate=0.100, error=23.929\n",
      ">epoch=357, lrate=0.100, error=23.182\n",
      ">epoch=358, lrate=0.100, error=22.446\n",
      ">epoch=359, lrate=0.100, error=21.722\n",
      ">epoch=360, lrate=0.100, error=21.013\n",
      ">epoch=361, lrate=0.100, error=20.321\n",
      ">epoch=362, lrate=0.100, error=19.648\n",
      ">epoch=363, lrate=0.100, error=18.995\n",
      ">epoch=364, lrate=0.100, error=18.364\n",
      ">epoch=365, lrate=0.100, error=17.755\n",
      ">epoch=366, lrate=0.100, error=17.169\n",
      ">epoch=367, lrate=0.100, error=16.607\n",
      ">epoch=368, lrate=0.100, error=16.069\n",
      ">epoch=369, lrate=0.100, error=15.555\n",
      ">epoch=370, lrate=0.100, error=15.063\n",
      ">epoch=371, lrate=0.100, error=14.595\n",
      ">epoch=372, lrate=0.100, error=14.150\n",
      ">epoch=373, lrate=0.100, error=13.726\n",
      ">epoch=374, lrate=0.100, error=13.323\n",
      ">epoch=375, lrate=0.100, error=12.941\n",
      ">epoch=376, lrate=0.100, error=12.579\n",
      ">epoch=377, lrate=0.100, error=12.235\n",
      ">epoch=378, lrate=0.100, error=11.909\n",
      ">epoch=379, lrate=0.100, error=11.600\n",
      ">epoch=380, lrate=0.100, error=11.307\n",
      ">epoch=381, lrate=0.100, error=11.030\n",
      ">epoch=382, lrate=0.100, error=10.766\n",
      ">epoch=383, lrate=0.100, error=10.517\n",
      ">epoch=384, lrate=0.100, error=10.280\n",
      ">epoch=385, lrate=0.100, error=10.056\n",
      ">epoch=386, lrate=0.100, error=9.843\n",
      ">epoch=387, lrate=0.100, error=9.640\n",
      ">epoch=388, lrate=0.100, error=9.448\n",
      ">epoch=389, lrate=0.100, error=9.265\n",
      ">epoch=390, lrate=0.100, error=9.091\n",
      ">epoch=391, lrate=0.100, error=8.926\n",
      ">epoch=392, lrate=0.100, error=8.768\n",
      ">epoch=393, lrate=0.100, error=8.618\n",
      ">epoch=394, lrate=0.100, error=8.475\n",
      ">epoch=395, lrate=0.100, error=8.339\n",
      ">epoch=396, lrate=0.100, error=8.209\n",
      ">epoch=397, lrate=0.100, error=8.084\n",
      ">epoch=398, lrate=0.100, error=7.966\n",
      ">epoch=399, lrate=0.100, error=7.852\n",
      ">epoch=400, lrate=0.100, error=7.743\n",
      ">epoch=401, lrate=0.100, error=7.639\n",
      ">epoch=402, lrate=0.100, error=7.540\n",
      ">epoch=403, lrate=0.100, error=7.444\n",
      ">epoch=404, lrate=0.100, error=7.352\n",
      ">epoch=405, lrate=0.100, error=7.265\n",
      ">epoch=406, lrate=0.100, error=7.180\n",
      ">epoch=407, lrate=0.100, error=7.099\n",
      ">epoch=408, lrate=0.100, error=7.021\n",
      ">epoch=409, lrate=0.100, error=6.946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=410, lrate=0.100, error=6.874\n",
      ">epoch=411, lrate=0.100, error=6.805\n",
      ">epoch=412, lrate=0.100, error=6.738\n",
      ">epoch=413, lrate=0.100, error=6.673\n",
      ">epoch=414, lrate=0.100, error=6.611\n",
      ">epoch=415, lrate=0.100, error=6.551\n",
      ">epoch=416, lrate=0.100, error=6.493\n",
      ">epoch=417, lrate=0.100, error=6.438\n",
      ">epoch=418, lrate=0.100, error=6.384\n",
      ">epoch=419, lrate=0.100, error=6.332\n",
      ">epoch=420, lrate=0.100, error=6.281\n",
      ">epoch=421, lrate=0.100, error=6.233\n",
      ">epoch=422, lrate=0.100, error=6.186\n",
      ">epoch=423, lrate=0.100, error=6.140\n",
      ">epoch=424, lrate=0.100, error=6.096\n",
      ">epoch=425, lrate=0.100, error=6.053\n",
      ">epoch=426, lrate=0.100, error=6.012\n",
      ">epoch=427, lrate=0.100, error=5.972\n",
      ">epoch=428, lrate=0.100, error=5.933\n",
      ">epoch=429, lrate=0.100, error=5.895\n",
      ">epoch=430, lrate=0.100, error=5.859\n",
      ">epoch=431, lrate=0.100, error=5.824\n",
      ">epoch=432, lrate=0.100, error=5.789\n",
      ">epoch=433, lrate=0.100, error=5.756\n",
      ">epoch=434, lrate=0.100, error=5.723\n",
      ">epoch=435, lrate=0.100, error=5.692\n",
      ">epoch=436, lrate=0.100, error=5.662\n",
      ">epoch=437, lrate=0.100, error=5.632\n",
      ">epoch=438, lrate=0.100, error=5.603\n",
      ">epoch=439, lrate=0.100, error=5.575\n",
      ">epoch=440, lrate=0.100, error=5.548\n",
      ">epoch=441, lrate=0.100, error=5.521\n",
      ">epoch=442, lrate=0.100, error=5.496\n",
      ">epoch=443, lrate=0.100, error=5.470\n",
      ">epoch=444, lrate=0.100, error=5.446\n",
      ">epoch=445, lrate=0.100, error=5.422\n",
      ">epoch=446, lrate=0.100, error=5.399\n",
      ">epoch=447, lrate=0.100, error=5.377\n",
      ">epoch=448, lrate=0.100, error=5.355\n",
      ">epoch=449, lrate=0.100, error=5.334\n",
      ">epoch=450, lrate=0.100, error=5.313\n",
      ">epoch=451, lrate=0.100, error=5.293\n",
      ">epoch=452, lrate=0.100, error=5.273\n",
      ">epoch=453, lrate=0.100, error=5.254\n",
      ">epoch=454, lrate=0.100, error=5.235\n",
      ">epoch=455, lrate=0.100, error=5.217\n",
      ">epoch=456, lrate=0.100, error=5.199\n",
      ">epoch=457, lrate=0.100, error=5.182\n",
      ">epoch=458, lrate=0.100, error=5.165\n",
      ">epoch=459, lrate=0.100, error=5.148\n",
      ">epoch=460, lrate=0.100, error=5.132\n",
      ">epoch=461, lrate=0.100, error=5.117\n",
      ">epoch=462, lrate=0.100, error=5.101\n",
      ">epoch=463, lrate=0.100, error=5.086\n",
      ">epoch=464, lrate=0.100, error=5.072\n",
      ">epoch=465, lrate=0.100, error=5.058\n",
      ">epoch=466, lrate=0.100, error=5.044\n",
      ">epoch=467, lrate=0.100, error=5.030\n",
      ">epoch=468, lrate=0.100, error=5.017\n",
      ">epoch=469, lrate=0.100, error=5.004\n",
      ">epoch=470, lrate=0.100, error=4.991\n",
      ">epoch=471, lrate=0.100, error=4.979\n",
      ">epoch=472, lrate=0.100, error=4.967\n",
      ">epoch=473, lrate=0.100, error=4.955\n",
      ">epoch=474, lrate=0.100, error=4.944\n",
      ">epoch=475, lrate=0.100, error=4.933\n",
      ">epoch=476, lrate=0.100, error=4.922\n",
      ">epoch=477, lrate=0.100, error=4.911\n",
      ">epoch=478, lrate=0.100, error=4.901\n",
      ">epoch=479, lrate=0.100, error=4.891\n",
      ">epoch=480, lrate=0.100, error=4.881\n",
      ">epoch=481, lrate=0.100, error=4.871\n",
      ">epoch=482, lrate=0.100, error=4.861\n",
      ">epoch=483, lrate=0.100, error=4.852\n",
      ">epoch=484, lrate=0.100, error=4.843\n",
      ">epoch=485, lrate=0.100, error=4.834\n",
      ">epoch=486, lrate=0.100, error=4.825\n",
      ">epoch=487, lrate=0.100, error=4.817\n",
      ">epoch=488, lrate=0.100, error=4.809\n",
      ">epoch=489, lrate=0.100, error=4.800\n",
      ">epoch=490, lrate=0.100, error=4.793\n",
      ">epoch=491, lrate=0.100, error=4.785\n",
      ">epoch=492, lrate=0.100, error=4.777\n",
      ">epoch=493, lrate=0.100, error=4.770\n",
      ">epoch=494, lrate=0.100, error=4.762\n",
      ">epoch=495, lrate=0.100, error=4.755\n",
      ">epoch=496, lrate=0.100, error=4.748\n",
      ">epoch=497, lrate=0.100, error=4.742\n",
      ">epoch=498, lrate=0.100, error=4.735\n",
      ">epoch=499, lrate=0.100, error=4.728\n"
     ]
    }
   ],
   "source": [
    "# normalize inputs\n",
    "\n",
    "minmax = dataset_minmax(trainingSet)\n",
    "normalize_data(trainingSet, minmax)\n",
    "\n",
    "learning_rate = 0.1\n",
    "n_epoch = 500\n",
    "n_hidden = 4 # neuron per layer\n",
    "\n",
    "back_propagate(trainingSet, learning_rate, n_epoch, n_hidden)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
