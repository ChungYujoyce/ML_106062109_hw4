{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from math import exp\n",
    "import matplotlib.pyplot as plt\n",
    "# process data label as 0, 1, 2 for training\n",
    "def loadDataset(dataset): \n",
    "    newdata = []\n",
    "    for x in range(len(dataset)-1):\n",
    "        for i in range(0,len(dataset[x]),4):\n",
    "            if dataset[x][i] == \"I\": # encounter labels, change it\n",
    "                if dataset[x][-3:-1] == \"sa\": # Iris-setosa\n",
    "                    newdata.append(0)\n",
    "                elif dataset[x][-2:-1] == \"r\": # Iris-versicolor\n",
    "                    newdata.append(1)\n",
    "                elif dataset[x][-3:-1] == \"ca\": # Iris-virginica\n",
    "                    newdata.append(2) \n",
    "                break\n",
    "            else:\n",
    "                attribute = float(dataset[x][i:i+3])\n",
    "                newdata.append(attribute)\n",
    "        trainingSet.append(newdata)       \n",
    "        newdata = [] # clear the package\n",
    "    return trainingSet\n",
    "\n",
    "# find the range of data to do normalize\n",
    "def dataset_minmax(dataset): # zip a(1,2,3) , b(4,5,6) to [(1,4), (2,5), (3,6)] \n",
    "    minmax = list()\n",
    "    stats = [[min(column), max(column)] for column in zip(*dataset)] # unzip the file\n",
    "    return stats\n",
    "\n",
    "# rescale data to range 0~1\n",
    "def normalize_data(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)-1):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "# calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    "\n",
    "# transfer neuron activation \n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + exp(-activation))\n",
    "\n",
    "# calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)\n",
    "\n",
    "#forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs # keep updating the neurons\n",
    "    return inputs\n",
    "\n",
    "# backpropagate error and sotre in neurons\n",
    "def backward_propagate_error(network, expected): \n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)): # hidden\n",
    "                error = 0.0\n",
    "                for neuron in network[i+1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error) \n",
    "        else:\n",
    "            for j in range(len(layer)): # output\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "\n",
    "# update network weights with error\n",
    "def update_weights(network, row, learning_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += learning_rate * neuron['delta'] * inputs[j]\n",
    "                neuron['weights'][-1] += learning_rate * neuron['delta']\n",
    "            \n",
    "def train_network(network, train, learning_rate, n_epoch, n_outputs):\n",
    "    print(\"--------------- lrate=%.3f\" % learning_rate ,\"-----------------\")\n",
    "    prev_MSE = 0\n",
    "    MSE = 0\n",
    "    abs_fraction_of_change = 0\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        MSE = 0\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1 # one hot encoding !!!\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, learning_rate)\n",
    "            update_outputs = forward_propagate(network, row) # update forward results\n",
    "            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "            MSE += sum([(expected[i] - update_outputs[i])**2 for i in range(len(expected))])\n",
    "            #print(MSE)\n",
    "        MSE /= len(expected)\n",
    "        if epoch > 0:\n",
    "            abs_fraction_of_change = abs((MSE - prev_MSE) / prev_MSE )\n",
    "            if abs_fraction_of_change <= 10e-5:\n",
    "                print(\"Epoch need:%d\"%(epoch+1))\n",
    "                break\n",
    "        print('>epoch=%d, MSE=%.3f, abs fraction of change=%.6f' % (epoch, MSE, abs_fraction_of_change))\n",
    "        #print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, learning_rate, sum_error))\n",
    "        prev_MSE = MSE\n",
    "        #print(\"prev\",prev_MSE)\n",
    "        \n",
    "        predicted = [predict(network, row) for row in train]\n",
    "        actual = [row[-1] for row in train]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        print(accuracy)\n",
    "    return (epoch+1)\n",
    "    \n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0    \n",
    "\n",
    "def init_network(n_inputs, n_hidden, n_outputs): \n",
    "# create n_hidden neurons and each neuron in the hidden layer has n_inputs + 1 weights\n",
    "    network = list() \n",
    "    hidden_layer1 = [{'weights': [(random.random()-0.5)/5.0 for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer1) \n",
    "    hidden_layer2 = [{'weights': [(random.random()-0.5)/5.0 for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer2)\n",
    "    output_layer = [{'weights': [(random.random()-0.5)/5.0  for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer) \n",
    "    return network\n",
    "\n",
    "# make a prediction with a network\n",
    "# It returns the index in the network output that has the largest probability. \n",
    "# Assuming that class values have been converted to integers starting at 0. [0,1,2]\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    #print(outputs)\n",
    "    return outputs.index(max(outputs))\n",
    "\n",
    "# backpropagation with stochastic gradient descent\n",
    "def back_propagate(train, learning_rate, n_epoch, n_hidden):\n",
    "    n_inputs = len(train[0]) - 1\n",
    "    n_outputs = len(set([row[-1] for row in train]))\n",
    "    network = init_network(n_inputs, n_hidden, n_outputs)\n",
    "    epoch = train_network(network, train, learning_rate, n_epoch, n_outputs)\n",
    "    return epoch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet=[]\n",
    "testSet=[]\n",
    "results = []\n",
    "f = open('iris.data.txt', \"r\")\n",
    "lines = f.readlines()\n",
    "dataset = list(lines)\n",
    "trainingSet = loadDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- lrate=0.500 -----------------\n",
      ">epoch=0, MSE=11.282, abs fraction of change=0.000000\n",
      "33.33333333333333\n",
      ">epoch=1, MSE=15.654, abs fraction of change=0.387519\n",
      "33.33333333333333\n",
      ">epoch=2, MSE=15.789, abs fraction of change=0.008657\n",
      "33.33333333333333\n",
      ">epoch=3, MSE=15.886, abs fraction of change=0.006109\n",
      "33.33333333333333\n",
      ">epoch=4, MSE=15.977, abs fraction of change=0.005708\n",
      "33.33333333333333\n",
      ">epoch=5, MSE=16.062, abs fraction of change=0.005321\n",
      "33.33333333333333\n",
      ">epoch=6, MSE=16.141, abs fraction of change=0.004941\n",
      "33.33333333333333\n",
      ">epoch=7, MSE=16.215, abs fraction of change=0.004569\n",
      "33.33333333333333\n",
      ">epoch=8, MSE=16.283, abs fraction of change=0.004207\n",
      "33.33333333333333\n",
      ">epoch=9, MSE=16.346, abs fraction of change=0.003857\n",
      "33.33333333333333\n",
      ">epoch=10, MSE=16.403, abs fraction of change=0.003523\n",
      "33.33333333333333\n",
      ">epoch=11, MSE=16.456, abs fraction of change=0.003207\n",
      "33.33333333333333\n",
      ">epoch=12, MSE=16.504, abs fraction of change=0.002915\n",
      "33.33333333333333\n",
      ">epoch=13, MSE=16.547, abs fraction of change=0.002648\n",
      "33.33333333333333\n",
      ">epoch=14, MSE=16.587, abs fraction of change=0.002407\n",
      "33.33333333333333\n",
      ">epoch=15, MSE=16.624, abs fraction of change=0.002193\n",
      "33.33333333333333\n",
      ">epoch=16, MSE=16.657, abs fraction of change=0.002002\n",
      "33.33333333333333\n",
      ">epoch=17, MSE=16.687, abs fraction of change=0.001832\n",
      "33.33333333333333\n",
      ">epoch=18, MSE=16.716, abs fraction of change=0.001681\n",
      "33.33333333333333\n",
      ">epoch=19, MSE=16.741, abs fraction of change=0.001546\n",
      "33.33333333333333\n",
      ">epoch=20, MSE=16.765, abs fraction of change=0.001424\n",
      "33.33333333333333\n",
      ">epoch=21, MSE=16.787, abs fraction of change=0.001314\n",
      "33.33333333333333\n",
      ">epoch=22, MSE=16.808, abs fraction of change=0.001215\n",
      "33.33333333333333\n",
      ">epoch=23, MSE=16.827, abs fraction of change=0.001125\n",
      "33.33333333333333\n",
      ">epoch=24, MSE=16.844, abs fraction of change=0.001042\n",
      "33.33333333333333\n",
      ">epoch=25, MSE=16.860, abs fraction of change=0.000967\n",
      "33.33333333333333\n",
      ">epoch=26, MSE=16.876, abs fraction of change=0.000899\n",
      "33.33333333333333\n",
      ">epoch=27, MSE=16.890, abs fraction of change=0.000836\n",
      "33.33333333333333\n",
      ">epoch=28, MSE=16.903, abs fraction of change=0.000778\n",
      "33.33333333333333\n",
      ">epoch=29, MSE=16.915, abs fraction of change=0.000726\n",
      "33.33333333333333\n",
      ">epoch=30, MSE=16.927, abs fraction of change=0.000678\n",
      "33.33333333333333\n",
      ">epoch=31, MSE=16.937, abs fraction of change=0.000634\n",
      "33.33333333333333\n",
      ">epoch=32, MSE=16.947, abs fraction of change=0.000593\n",
      "33.33333333333333\n",
      ">epoch=33, MSE=16.957, abs fraction of change=0.000556\n",
      "33.33333333333333\n",
      ">epoch=34, MSE=16.966, abs fraction of change=0.000522\n",
      "33.33333333333333\n",
      ">epoch=35, MSE=16.974, abs fraction of change=0.000491\n",
      "33.33333333333333\n",
      ">epoch=36, MSE=16.982, abs fraction of change=0.000462\n",
      "33.33333333333333\n",
      ">epoch=37, MSE=16.989, abs fraction of change=0.000435\n",
      "33.33333333333333\n",
      ">epoch=38, MSE=16.996, abs fraction of change=0.000410\n",
      "33.33333333333333\n",
      ">epoch=39, MSE=17.003, abs fraction of change=0.000388\n",
      "33.33333333333333\n",
      ">epoch=40, MSE=17.009, abs fraction of change=0.000367\n",
      "33.33333333333333\n",
      ">epoch=41, MSE=17.015, abs fraction of change=0.000347\n",
      "33.33333333333333\n",
      ">epoch=42, MSE=17.020, abs fraction of change=0.000330\n",
      "33.33333333333333\n",
      ">epoch=43, MSE=17.026, abs fraction of change=0.000313\n",
      "33.33333333333333\n",
      ">epoch=44, MSE=17.031, abs fraction of change=0.000297\n",
      "33.33333333333333\n",
      ">epoch=45, MSE=17.036, abs fraction of change=0.000283\n",
      "33.33333333333333\n",
      ">epoch=46, MSE=17.040, abs fraction of change=0.000270\n",
      "33.33333333333333\n",
      ">epoch=47, MSE=17.045, abs fraction of change=0.000257\n",
      "33.33333333333333\n",
      ">epoch=48, MSE=17.049, abs fraction of change=0.000245\n",
      "33.33333333333333\n",
      ">epoch=49, MSE=17.053, abs fraction of change=0.000234\n",
      "33.33333333333333\n",
      ">epoch=50, MSE=17.057, abs fraction of change=0.000224\n",
      "33.33333333333333\n",
      ">epoch=51, MSE=17.060, abs fraction of change=0.000214\n",
      "33.33333333333333\n",
      ">epoch=52, MSE=17.064, abs fraction of change=0.000205\n",
      "33.33333333333333\n",
      ">epoch=53, MSE=17.067, abs fraction of change=0.000197\n",
      "33.33333333333333\n",
      ">epoch=54, MSE=17.070, abs fraction of change=0.000188\n",
      "33.33333333333333\n",
      ">epoch=55, MSE=17.073, abs fraction of change=0.000181\n",
      "33.33333333333333\n",
      ">epoch=56, MSE=17.076, abs fraction of change=0.000174\n",
      "33.33333333333333\n",
      ">epoch=57, MSE=17.079, abs fraction of change=0.000167\n",
      "33.33333333333333\n",
      ">epoch=58, MSE=17.082, abs fraction of change=0.000160\n",
      "33.33333333333333\n",
      ">epoch=59, MSE=17.085, abs fraction of change=0.000154\n",
      "33.33333333333333\n",
      ">epoch=60, MSE=17.087, abs fraction of change=0.000148\n",
      "33.33333333333333\n",
      ">epoch=61, MSE=17.090, abs fraction of change=0.000143\n",
      "33.33333333333333\n",
      ">epoch=62, MSE=17.092, abs fraction of change=0.000137\n",
      "33.33333333333333\n",
      ">epoch=63, MSE=17.094, abs fraction of change=0.000132\n",
      "33.33333333333333\n",
      ">epoch=64, MSE=17.096, abs fraction of change=0.000128\n",
      "33.33333333333333\n",
      ">epoch=65, MSE=17.098, abs fraction of change=0.000123\n",
      "33.33333333333333\n",
      ">epoch=66, MSE=17.100, abs fraction of change=0.000119\n",
      "33.33333333333333\n",
      ">epoch=67, MSE=17.102, abs fraction of change=0.000115\n",
      "33.33333333333333\n",
      ">epoch=68, MSE=17.104, abs fraction of change=0.000111\n",
      "33.33333333333333\n",
      ">epoch=69, MSE=17.106, abs fraction of change=0.000107\n",
      "33.33333333333333\n",
      ">epoch=70, MSE=17.108, abs fraction of change=0.000103\n",
      "33.33333333333333\n",
      ">epoch=71, MSE=17.110, abs fraction of change=0.000100\n",
      "33.33333333333333\n",
      "Epoch need:73\n"
     ]
    }
   ],
   "source": [
    "# normalize inputs\n",
    "\n",
    "minmax = dataset_minmax(trainingSet)\n",
    "normalize_data(trainingSet, minmax)\n",
    "\n",
    "learning_rate = 0.5\n",
    "n_epoch = 700\n",
    "n_hidden = 4\n",
    "scores = list()\n",
    "epoch = back_propagate(trainingSet, learning_rate, n_epoch, n_hidden)\n",
    "#results.append(epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'weights': [-0.09286680684125943, -0.07366464786970842, -0.03666755705500983, 0.026152292681586385, -0.07581125392629509]}, {'weights': [-0.03671509321507198, 0.0900139360309646, -0.0019825385987958465, -0.08501619274988252, 0.0418844948029351]}, {'weights': [0.002351024314798167, -0.07429909575372226, -0.08421556427057572, 0.022054399358507525, -0.05606397310133526]}, {'weights': [0.03284638318118562, -0.03860530601534373, 0.07499440332684693, 0.04925405852776825, -0.07744077010039177]}]\n",
      "[{'weights': [0.23420986118579412, 0.8192084468788082, 0.2241627686288864, 0.8130485040509653, 0.0651783394736023]}, {'weights': [0.7077039440261911, 0.5809014115734785, 0.8496781145246343, 0.765093112828106, 0.46455759724446366]}, {'weights': [0.44094999244348476, 0.3080109660718007, 0.7286375809951946, 0.7705247135245551, 0.7397995517227348]}, {'weights': [0.8044553260165456, 0.968794829688238, 0.8226139259627132, 0.6175617466905798, 0.30804238521590466]}]\n",
      "[{'weights': [0.06253107941983851, -0.08974679598099597, -0.028267643578388114, 0.026009149986545, -0.052424077783249576]}, {'weights': [0.0036564271605485253, -0.034744476323303755, -0.05570432916658925, -0.09688285578549069, 0.07763153174852247]}, {'weights': [-0.004747557061024477, 0.08106251805061271, 0.09268796361736449, -0.09102585804078223, -0.08529748349180724]}]\n",
      "--------------- lrate=0.200 -----------------\n",
      ">epoch=0, lrate=0.200, error=114.704\n",
      "33.33333333333333\n",
      ">epoch=1, lrate=0.200, error=130.369\n",
      "33.33333333333333\n",
      ">epoch=2, lrate=0.200, error=130.694\n",
      "33.33333333333333\n",
      ">epoch=3, lrate=0.200, error=131.033\n",
      "33.33333333333333\n",
      ">epoch=4, lrate=0.200, error=131.391\n",
      "33.33333333333333\n",
      ">epoch=5, lrate=0.200, error=131.768\n",
      "33.33333333333333\n",
      ">epoch=6, lrate=0.200, error=132.164\n",
      "33.33333333333333\n",
      ">epoch=7, lrate=0.200, error=132.581\n",
      "33.33333333333333\n",
      ">epoch=8, lrate=0.200, error=133.018\n",
      "33.33333333333333\n",
      ">epoch=9, lrate=0.200, error=133.475\n",
      "33.33333333333333\n",
      ">epoch=10, lrate=0.200, error=133.953\n",
      "33.33333333333333\n",
      ">epoch=11, lrate=0.200, error=134.451\n",
      "33.33333333333333\n",
      ">epoch=12, lrate=0.200, error=134.969\n",
      "33.33333333333333\n",
      ">epoch=13, lrate=0.200, error=135.507\n",
      "33.33333333333333\n",
      ">epoch=14, lrate=0.200, error=136.062\n",
      "33.33333333333333\n",
      ">epoch=15, lrate=0.200, error=136.634\n",
      "33.33333333333333\n",
      ">epoch=16, lrate=0.200, error=137.222\n",
      "33.33333333333333\n",
      ">epoch=17, lrate=0.200, error=137.822\n",
      "33.33333333333333\n",
      ">epoch=18, lrate=0.200, error=138.433\n",
      "33.33333333333333\n",
      ">epoch=19, lrate=0.200, error=139.051\n",
      "33.33333333333333\n",
      ">epoch=20, lrate=0.200, error=139.672\n",
      "33.33333333333333\n",
      ">epoch=21, lrate=0.200, error=140.292\n",
      "33.33333333333333\n",
      ">epoch=22, lrate=0.200, error=140.908\n",
      "33.33333333333333\n",
      ">epoch=23, lrate=0.200, error=141.514\n",
      "33.33333333333333\n",
      ">epoch=24, lrate=0.200, error=142.107\n",
      "33.33333333333333\n",
      ">epoch=25, lrate=0.200, error=142.685\n",
      "33.33333333333333\n",
      ">epoch=26, lrate=0.200, error=143.244\n",
      "33.33333333333333\n",
      ">epoch=27, lrate=0.200, error=143.783\n",
      "33.33333333333333\n",
      ">epoch=28, lrate=0.200, error=144.302\n",
      "33.33333333333333\n",
      ">epoch=29, lrate=0.200, error=144.801\n",
      "33.33333333333333\n",
      ">epoch=30, lrate=0.200, error=145.281\n",
      "33.33333333333333\n",
      ">epoch=31, lrate=0.200, error=145.743\n",
      "33.33333333333333\n",
      ">epoch=32, lrate=0.200, error=146.186\n",
      "33.33333333333333\n",
      ">epoch=33, lrate=0.200, error=146.613\n",
      "33.33333333333333\n",
      ">epoch=34, lrate=0.200, error=147.023\n",
      "33.33333333333333\n",
      ">epoch=35, lrate=0.200, error=147.416\n",
      "33.33333333333333\n",
      ">epoch=36, lrate=0.200, error=147.793\n",
      "33.33333333333333\n",
      ">epoch=37, lrate=0.200, error=148.152\n",
      "33.33333333333333\n",
      ">epoch=38, lrate=0.200, error=148.494\n",
      "33.33333333333333\n",
      ">epoch=39, lrate=0.200, error=148.817\n",
      "33.33333333333333\n",
      ">epoch=40, lrate=0.200, error=149.121\n",
      "33.33333333333333\n",
      ">epoch=41, lrate=0.200, error=149.404\n",
      "33.33333333333333\n",
      ">epoch=42, lrate=0.200, error=149.667\n",
      "33.33333333333333\n",
      ">epoch=43, lrate=0.200, error=149.909\n",
      "33.33333333333333\n",
      ">epoch=44, lrate=0.200, error=150.131\n",
      "33.33333333333333\n",
      ">epoch=45, lrate=0.200, error=150.334\n",
      "33.33333333333333\n",
      ">epoch=46, lrate=0.200, error=150.520\n",
      "33.33333333333333\n",
      ">epoch=47, lrate=0.200, error=150.689\n",
      "33.33333333333333\n",
      ">epoch=48, lrate=0.200, error=150.843\n",
      "33.33333333333333\n",
      ">epoch=49, lrate=0.200, error=150.984\n",
      "33.33333333333333\n",
      ">epoch=50, lrate=0.200, error=151.113\n",
      "33.33333333333333\n",
      ">epoch=51, lrate=0.200, error=151.231\n",
      "33.33333333333333\n",
      ">epoch=52, lrate=0.200, error=151.340\n",
      "33.33333333333333\n",
      ">epoch=53, lrate=0.200, error=151.440\n",
      "33.33333333333333\n",
      ">epoch=54, lrate=0.200, error=151.532\n",
      "33.33333333333333\n",
      ">epoch=55, lrate=0.200, error=151.618\n",
      "33.33333333333333\n",
      ">epoch=56, lrate=0.200, error=151.697\n",
      "33.33333333333333\n",
      ">epoch=57, lrate=0.200, error=151.770\n",
      "33.33333333333333\n",
      ">epoch=58, lrate=0.200, error=151.838\n",
      "33.33333333333333\n",
      ">epoch=59, lrate=0.200, error=151.902\n",
      "33.33333333333333\n",
      ">epoch=60, lrate=0.200, error=151.962\n",
      "33.33333333333333\n",
      ">epoch=61, lrate=0.200, error=152.018\n",
      "33.33333333333333\n",
      ">epoch=62, lrate=0.200, error=152.071\n",
      "33.33333333333333\n",
      ">epoch=63, lrate=0.200, error=152.121\n",
      "33.33333333333333\n",
      ">epoch=64, lrate=0.200, error=152.170\n",
      "33.33333333333333\n",
      ">epoch=65, lrate=0.200, error=152.216\n",
      "33.33333333333333\n",
      ">epoch=66, lrate=0.200, error=152.262\n",
      "33.33333333333333\n",
      ">epoch=67, lrate=0.200, error=152.306\n",
      "33.33333333333333\n",
      ">epoch=68, lrate=0.200, error=152.349\n",
      "33.33333333333333\n",
      ">epoch=69, lrate=0.200, error=152.392\n",
      "33.33333333333333\n",
      ">epoch=70, lrate=0.200, error=152.434\n",
      "33.33333333333333\n",
      ">epoch=71, lrate=0.200, error=152.476\n",
      "33.33333333333333\n",
      ">epoch=72, lrate=0.200, error=152.518\n",
      "33.33333333333333\n",
      ">epoch=73, lrate=0.200, error=152.560\n",
      "33.33333333333333\n",
      ">epoch=74, lrate=0.200, error=152.601\n",
      "33.33333333333333\n",
      ">epoch=75, lrate=0.200, error=152.643\n",
      "33.33333333333333\n",
      ">epoch=76, lrate=0.200, error=152.685\n",
      "33.33333333333333\n",
      ">epoch=77, lrate=0.200, error=152.727\n",
      "33.33333333333333\n",
      ">epoch=78, lrate=0.200, error=152.769\n",
      "33.33333333333333\n",
      ">epoch=79, lrate=0.200, error=152.810\n",
      "33.33333333333333\n",
      ">epoch=80, lrate=0.200, error=152.851\n",
      "33.33333333333333\n",
      ">epoch=81, lrate=0.200, error=152.892\n",
      "33.33333333333333\n",
      ">epoch=82, lrate=0.200, error=152.932\n",
      "33.33333333333333\n",
      ">epoch=83, lrate=0.200, error=152.971\n",
      "33.33333333333333\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5cfdd8ed3876>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mn_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mback_propagate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-5d3ef2b59981>\u001b[0m in \u001b[0;36mback_propagate\u001b[1;34m(train, learning_rate, n_epoch, n_hidden)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[0mn_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[0mnetwork\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-5d3ef2b59981>\u001b[0m in \u001b[0;36mtrain_network\u001b[1;34m(network, train, learning_rate, n_epoch, n_outputs)\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[0mexpected\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m# one hot encoding !!!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mbackward_propagate_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             \u001b[0mupdate_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m             \u001b[0mupdate_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_propagate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# update forward results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[0msum_error\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpected\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-5d3ef2b59981>\u001b[0m in \u001b[0;36mupdate_weights\u001b[1;34m(network, row, learning_rate)\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m                 \u001b[0mneuron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weights'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mneuron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'delta'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m                 \u001b[0mneuron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weights'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mneuron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'delta'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# normalize inputs\n",
    "dataset = list(lines)\n",
    "trainingSet = loadDataset(dataset) #reload data\n",
    "minmax = dataset_minmax(trainingSet)\n",
    "normalize_data(trainingSet, minmax)\n",
    "\n",
    "learning_rate = 0.2\n",
    "n_epoch = 300\n",
    "n_hidden = 4\n",
    "\n",
    "epoch = back_propagate(trainingSet, learning_rate, n_epoch, n_hidden)\n",
    "results.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs\n",
    "trainingSet = loadDataset(dataset) #reload data\n",
    "minmax = dataset_minmax(trainingSet)\n",
    "normalize_data(trainingSet, minmax)\n",
    "\n",
    "learning_rate = 0.3\n",
    "n_epoch = 300\n",
    "n_hidden = 4\n",
    "\n",
    "epoch = back_propagate(trainingSet, learning_rate, n_epoch, n_hidden)\n",
    "results.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs\n",
    "trainingSet = loadDataset(dataset) #reload data\n",
    "minmax = dataset_minmax(trainingSet)\n",
    "normalize_data(trainingSet, minmax)\n",
    "\n",
    "learning_rate = 0.4\n",
    "n_epoch = 300\n",
    "n_hidden = 4\n",
    "\n",
    "epoch = back_propagate(trainingSet, learning_rate, n_epoch, n_hidden)\n",
    "results.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs\n",
    "trainingSet = loadDataset(dataset) #reload data\n",
    "minmax = dataset_minmax(trainingSet)\n",
    "normalize_data(trainingSet, minmax)\n",
    "\n",
    "learning_rate = 0.5\n",
    "n_epoch = 300\n",
    "n_hidden = 4\n",
    "\n",
    "epoch = back_propagate(trainingSet, learning_rate, n_epoch, n_hidden)\n",
    "results.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([0.1, 0.2, 0.3, 0.4, 0.5], results, 'r')\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Number of epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
